Finally, we can start building the model using the computational modules. Some commonly used computational modules are:

\begin{table}
\begin{tabular}{l l} \hline \hline Notations & Descriptions \\ \hline \hline \(\mathbf{g}^{n}\) & \(m\)-dimensional Euclidean space \\ \(\alpha,\mathbf{n},\mathbf{A}\) & Scalar, vector and matrix \\ \(\mathbf{A}^{r}\) & Matrix transpose \\ \(\mathbf{l}_{v}\) & Identity matrix of dimension \(N\) \\ \(\mathbf{g}_{v}\) **x** & Convolution of \(\mathbf{g}_{v}\) and \(\mathbf{s}\) \\ \(N,N^{r}\) & Number of nodes in the graph \\ \(N^{r}\) & Number of edges in the graph \\ \(\cdot,r\) & Neighborhood set of node \\ \(\mathbf{s}^{r}\) & Vector \(a\) of node \(v\) at time step \(t\) \\ \(\mathbf{h}_{v}\) & Hidden state of node \(v\) \\ \(\mathbf{h}^{r}_{i}\) & Hidden state of node \(v\) at time step \(t\) \\ \(\mathbf{o}^{r}_{i}\) & Output of node \(v\) at time step \(t\) \\ \(\mathbf{e}_{vw}\) & Features of edge from node \(v\) to \(w\) \\ \(\mathbf{n}_{v}\) & Features of edge with label \(k\) \\ \(\mathbf{W}^{r}\),\(\mathbf{U}^{r}\) & Matrices for computing i, a \\ \(\mathbf{W}^{r}\),\(\mathbf{U}^{r}\) & Vectors for computing i, a \\ \(\rho\) & An alternative non-linear function \\ \(\sigma\) & The logistic sigmoid function \\ tanh & The hyperbolic tangent function \\ LealeyReLU & The LealeyReLU function \\ \(\odot\) & Element-wise multiplication operation \\ \(\parallel\) & Vector concatenation \\ \hline \hline \end{tabular}
\end{table}
Table 1: Notations used in this paper.

* **Propagation Module.** The propagation module is used to propagate information between nodes so that the aggregated information could capture both feature and topological information. In propagation modules, the **convolution operator** and **recurrent operator** are usually used to aggregate information from neighbors while the **skip connection** operation is used to gather information from historical representations of nodes and mitigate the over-smoothing problem.
* **Sampling Module.** When graphs are large, sampling modules are usually needed to conduct propagation on graphs. The sampling module is usually combined with the propagation module.
* **Pooling Module.** When we need the representations of high-level subgraphs or graphs, pooling modules are needed to extract information from nodes.

With these computation modules, a typical GNN model is usually built by combining them. A typical architecture of the GNN model is illustrated in the middle part of Fig. 2 where the convolutional operator, recurrent operator, sampling module and skip connection are used to propagate information in each layer and then the pooling module is added to extract high-level information. These layers are usually stacked to obtain better representations. Note this architecture can generalize most GNN models while there are also exceptions, for example, NDCN (Zang and Wang, 2020) combines ordinary differential equation systems (ODEs) and GNNs. It can be regarded as a continuous-time GNN model which integrates GNN layers over continuous time without propagating through a discrete number of layers.

An illustration of the general design pipeline is shown in Fig. 2. In later sections, we first give the existing instantiations of computational modules in Section 3, then introduce existing variants which consider different graph types and scale in Section 4. Then we survey on variants designed for different training settings in Section 5. These sections correspond to details of step (4), step (2), and step (3) in the pipeline. And finally, we give a concrete design example in Section 6.