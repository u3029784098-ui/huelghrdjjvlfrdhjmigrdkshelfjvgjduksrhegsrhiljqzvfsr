In a graph, each node is naturally defined by its features and the related nodes. The target of GNN is to learn a state embedding \(\mathbf{h}_{v}\in\mathbb{R}^{d}\) which contains the information of the neighborhood and itself for each node. The state embedding \(\mathbf{h}_{v}\) is an \(s\)-dimension vector of node \(v\) and can be used to produce an output \(\mathbf{o}_{v}\) such as the distribution of the predicted node label. Then the computation steps of \(\mathbf{h}_{v}\) and \(\mathbf{o}_{v}\) are defined as:

\[\mathbf{h}_{v}=f\left(\mathbf{x}_{v},\mathbf{x}_{v|i}\right)\mathbf{h}_{v^{ \prime}_{v}},\mathbf{x}_{v^{\prime}_{v}}\,, \tag{19}\]

\(\mathbf{o}_{v}=g(\mathbf{h}_{v},\mathbf{x}_{v})\),

where \(\mathbf{x}_{v},\mathbf{x}_{v|i},\mathbf{h}_{v^{\prime}_{v}},\mathbf{x}_{v^{ \prime}_{v}}\) are the features of \(\mathbf{v}\), the features of its edges, the states and the features of the nodes in the neighborhood of \(\mathbf{v}\), respectively. \(f\) here is a parametric function called the _local transition function_. It is shared among all nodes and updates the node state according to the input neighborhood. \(g\) is the _local output function_ that describes how the output is produced. Note that both \(f\) and \(g\) can be interpreted as the feedforward neural networks.

Let \(\mathbf{H}\), \(\mathbf{0}\), \(\mathbf{X}\), and \(\mathbf{X}_{N}\) be the matrices constructed by stacking all the states, all the outputs, all the features, and all the node features, respectively. Then we have a compact form as:

\[\mathbf{H}=F(\mathbf{H},\mathbf{X}), \tag{20}\] \[\mathbf{O}=G(\mathbf{H},\mathbf{X}_{N}),\]

where \(F\), the _global transition function_, and \(G\), the _global output function_ are stacked versions of \(f\) and \(g\) for all nodes in a graph, respectively. The value of \(\mathbf{H}\) is the fixed point of Eq. (20) and is uniquely defined with the assumption that \(F\) is a contraction map.

With the suggestion of Banach's fixed point theorem (Khamsi and Kirk, 2011), GNN uses the following classic iterative scheme to compute the state:

\[\mathbf{H}^{+1}=F(\mathbf{H}^{\prime},\mathbf{X}), \tag{21}\]

where \(\mathbf{H}^{\prime}\) denotes the \(t\)-th iteration of \(\mathbf{H}\). The dynamical system Eq. (21) converges exponentially fast to the solution for any initial value.

Though experimental results have shown that GNN is a powerful architecture for modeling structural data, there are still several limitations:

* GNN requires \(f\) to be a contraction map which limits the model's ability. And it is inefficient to update the hidden states of nodes iteratively towards the fixed point.
* It is unsuitable to use the fixed points if we focus on the representation of nodes instead of graphs because the distribution of representation in the fixed point will be much smoother in value and less informative for distinguishing each node.

GraphESNGraph echo state network (GraphESN) (Gallicchio and Micheli, 2010) generalizes the echo state network (ESN) (Jaeger, 2001) on graphs. It uses a fixed contractive encoding function, and only trains a readout function. The convergence is ensured by the contractivity of reservoir dynamics. As a consequence, GraphESN is more efficient than GNN.

SSEStochastic Steady-state Embedding (SSE) (Dai et al., 2018) is also proposed to improve the efficiency of GNN. SSE proposes a learning framework which contains two steps. Embeddings of each node are updated by a parameterized operator in the update step and these embeddings are projected to the steady state constraint space to meet the steady-state conditions.

LP-GNNLagrangian Propagation GNN (LP-GNN) (Tiezzi et al., 2020) formalizes the learning task as a constraint optimization problem in the Lagrangian framework and avoids the iterative computations for the fixed point. The convergence procedure is implicitly expressed by a constraint satisfaction mechanism.