There are several works attempting to use the gate mechanism like GRU (Cho et al., 2014) or LSTM (Hochreiter and Schmidhuber, 1997) in the propagation step to diminish the computational limitations in GNN and improve the long-term propagation of information across the graph structure. They run a fixed number of training steps without the guarantee of convergence.

GGNNThe gated graph neural network (GGNN) (Li et al., 2016) is proposed to release the limitations of GNN. It releases the requirement of function \(f\) to be a contraction map and uses the Gate Recurrent Units (GRU) in the propagation step. It also uses back-propagation through time (BPTT) to compute gradients. The computation step of GGNN can be found in Table 2.

The node \(\nu\) first aggregates messages from its neighbors. Then the GRU-like update functions incorporate information from the other nodes and from the previous timestep to update each node's hidden state. \(\mathbf{h}_{v^{\prime}_{v}}\), gathers the neighborhood information of node \(v\), while \(\mathbf{z}\) and \(\mathbf{r}\) are the update and reset gates.

LSTMs are also used in a similar way as GRU through the propagation process based on a tree or a graph.

TreeLSTM.Tai et al. (2015) propose two extensions on the tree structure to the basic LSTM architecture: the _Child-Sum Tree-LSTM_ and

\begin{table}
\begin{tabular}{l l l} Variant & Aggregator & Updater \\ \hline GGNN & \(\mathbf{h}^{\prime}_{v^{\prime}_{v^{\prime}_{v}}}-\sum\limits_{k\in\mathcal{V }_{v^{\prime}_{v}}}\mathbf{h}^{-1}_{v^{\prime}_{v}}+\mathbf{b}\) & \(\mathbf{\zeta}_{v}=\sigma(\mathbf{W}\mathbf{h}^{\prime}_{v^{\prime}_{v^{\prime} _{v}}}+\mathbf{U}\mathbf{h}^{-1}_{v^{\prime}_{v}})\) \\  & \(\mathbf{\zeta}_{v}=\sigma(\mathbf{W}\mathbf{h}^{\prime}_{v^{\prime}_{v^{\prime} _{v}}}+\mathbf{U}\mathbf{h}^{\prime}_{v^{\prime}_{v}})\) \\  & & \(\mathbf{h}^{\prime}_{v}=\tanh(\mathbf{W}\mathbf{h}^{\prime}_{v^{\prime}_{v}}+ \mathbf{U}\mathbf{h}^{\prime}_{v^{\prime}_{v}})\) \\  & & \(\mathbf{h}^{\prime}_{v}=(1-\mathbf{z})\odot\mathbf{h}^{\prime}_{v^{\prime}_{v}}+ \zeta\odot\mathbf{h}^{\prime}_{v^{\prime}_{v}}\) \\ \hline Tree LSTM (Child sum) & \(\mathbf{h}^{\prime}_{v^{\prime}_{v}}-\sum\limits_{k\in\mathcal{V}_{v^{\prime}_{v}}} \mathbf{h}^{-1}_{v^{\prime}_{v}}\) & \(\mathbf{\zeta}_{v}=\sigma(\mathbf{W}^{\prime}_{v^{\prime}_{v}}+\mathbf{b}^{ \prime}_{v^{\prime}_{v}}+\mathbf{b}^{\prime}_{v^{\prime}_{v}})\) \\  & & \(\mathbf{h}^{\prime}_{v^{\prime}_{v}}-\sum\limits_{k\in\mathcal{V}_{v^{\prime}_{v}}} \mathbf{h}^{-1}_{v^{\prime}_{v}}\) \\  & & \(\mathbf{h}^{\prime}_{v^{\prime}_{v}}-\sum\limits_{k\in\mathcal{V}_{v^{\prime}_{v}}} \mathbf{h}^{-1}_{v^{\prime}_{v}}\) \\  & & \(\mathbf{h}^{\prime}_{v^{\prime}_{v}}-\sum\limits_{k\in\mathcal{V}_{v^{\prime}_{v}}} \mathbf{h}^{-1}_{v^{\prime}_{v}}\) \\  & & \(\mathbf{h}^{\prime}_{v^{\prime}_{v}}-\sum\limits_{k\in\mathcal{V}_{v^{\prime}_{v}}} \mathbf{h}^{-1}_{v^{\prime}_{v}}\) \\  & & \(\mathbf{h}^{\prime}_{v^{\prime}_{v}}-\sum\limits_{k\in\mathcal{V}_{v^{\prime}_{v}}} \mathbf{h}^{-1}_{v^{\prime}_{v}}\) \\ \hline Tree LSTM (N-ary) & \(\mathbf{h}^{\prime}_{v^{\prime}_{v}}=\frac{\mathbf{x}}{\sum\limits_{k\in\mathcal{V}_{v^{ \prime}_{v}}}}\mathbf{h}^{-1}_{v^{\prime}_{v}}\) \\  & & \(\mathbf{h}^{\prime}_{v^{\prime}_{v}}-\sum\limits_{k\in\mathcal{V}_{v^{\prime}_{v}}} \mathbf{h}^{-1}_{v^{\prime}_{v}}\) \\  & & \(\mathbf{h}^{\prime}_{v^{\prime}_{v}}-\sum\limits_{k\in\mathcal{V}_{v^{\prime}_{v}}} \mathbf{h}^{-1}_{v^{\prime}_{v}}\) \\ Graph LSTM (Fung et al., 2017) & \(\mathbf{h}^{\prime}_{v^{\prime}_{v}}-\sum\limits_{k\in\mathcal{V}_{v^{\prime}_{v}}} \mathbf{h}^{-1}_{v^{\prime}_{v^{\prime}_{v}}}\) \\  & & \(\mathbf{h}^{\prime}_{v^{\prime}_{v^the _N-ary Tree-LSTM_. They are also extensions to the recursive neural network based models as we mentioned before. Tree is a special case of graph and each node in Tree-LSTM aggregates information from its children. Instead of a single forget gate in traditional LSTM, the Tree-LSTM unit for node \(v\) contains one forget gate \(\mathbf{f}_{v}\) for each child \(k\). The computation step of the Child-Sum Tree-LSTM is displayed in Table 2. \(\mathbf{f}_{v}^{t}\), \(\mathbf{o}_{v}^{t}\), and \(\mathbf{c}_{v}^{t}\) are the input gate, output gate and memory cell respectively. \(\mathbf{x}_{v}^{t}\) is the input vector at time \(t\). The _N-ary_ Tree-LSTM is further designed for a special kind of tree where each node has at most \(K\) children and the children are ordered. The equations for computing \(\mathbf{h}_{v,v}^{t}\), \(\mathbf{h}_{v,v}^{t}\), \(\mathbf{h}_{v,v}^{t}\), in Table 2 introduce separate parameters for each child \(k\). These parameters allow the model to learn more fine-grained representations conditioning on the states of a unit's children than the Child-Sum Tree-LSTM.

_Graph LSTM_. The two types of Tree-LSTMs can be easily adapted to the graph. The graph-structured LSTM in (Zayats and Ostendorf, 2018) is an example of the _N-ary_ Tree-LSTM applied to the graph. However, it is a simplified version since each node in the graph has at most 2 incoming edges (from its parent and sibling predecessor). Peng et al. (2017) propose another variant of the Graph LSTM based on the relation extraction task. The edges of graphs in (Peng et al., 2017) have various labels so that Peng et al. (2017) utilize different weight matrices to represent different labels. In Table 2, \(m(v,k)\) denotes the edge label between node \(v\) and \(k\). Liang et al. (2016) propose a Graph LSTM network to address the semantic object parsing task. It uses the confidence-driven scheme to adaptively select the starting node and determine the node updating sequence. It follows the same idea of generalizing the existing LSTMs into the graph-structured data but has a specific updating sequence while methods mentioned above are agnostic to the order of nodes.

_S-LSTM_. Zhang et al. (2018) propose Sentence LSTM (S-LSTM) for improving text encoding. It converts text into a graph and utilizes the Graph LSTM to learn the representation. The S-LSTM shows strong representation power in many NLP problems.