From the spectral perspective of view, GCNs perform convolution operation on the input features in the spectral domain, which follows graph signal processing in theory.

There exists several works analyzing GNNs from graph signal processing. Li et al. (2018c) first address the graph convolution in graph neural networks is actually Laplacian smoothing, which smooths the feature matrix so that nearby nodes have similar hidden representations. Laplacian smoothing reflects the homophily assumption that nearby nodes are supposed to be similar. The Laplacian matrix serves as a low-pass filter for the input features. SGC (Wu et al., 2019b) further removes the weight matrices and nonlinearities between layers, showing that the low-pass filter is the reason why GNNs work.

Following the idea of low-pass filtering, Zhang et al. (2019c), Cui et al. (2020), NT and Maehara (19), Chen et al. (2020b) analyze different filters and provide new insights. To achieve low-pass filtering for all the eigenvalues, AGC (Zhang et al., 2019c) designs a graph filter \(I-\frac{1}{2}L\) according to the frequency response function. AGE (Cui et al., 2020) further demonstrates that filter with \(I-\frac{1}{\alpha\alpha}L\) could get better results, where \(\lambda_{\text{max}}\) is the maximum eigenvalue of the Laplacian matrix. Despite linear filters, GraphHeat (Xuetal et al., 2019a) leverages heat kernels for better low-pass properties. NT and Maehara (Net and Maehara, 2019) state that graph convolution is mainly a denoising process for input features, the model performances heavily depend on the amount of noises in the feature matrix. To alleviate the over-smoothing issue, Chen et al. (2020b) present two metrics for measuring the smoothness of node representations and the over-smoothness of GNN models. The authors conclude that the information-to-noise ratio is the key factor for over-smoothing.