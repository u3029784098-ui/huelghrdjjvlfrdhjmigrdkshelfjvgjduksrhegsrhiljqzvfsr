hat graph convolution is mainly a denoising process for input features, the model performances heavily depend on the amount of noises in the feature matrix. To alleviate the over-smoothing issue, Chen et al. (2020b) present two metrics for measuring the smoothness of node representations and the over-smoothness of GNN models. The authors conclude that the information-to-noise ratio is the key factor for over-smoothing.

#### 7.1.2 Generalization

The generalization ability of GNNs have also received attentions recently. Scarselli et al. (2018) prove the VC-dimensions for a limited class of GNNs. Garg et al. (2020) further give much tighter generalization bounds based on Rademacher bounds for neural networks.

Verma and Zhang (2019) analyze the stability and generalization properties of single-layer GNNs with different convolutional filters. The authors conclude that the stability of GNNs depends on the largest eigenvalue of the filters. Knyazev et al. (2019) focus on the generalization ability of attention mechanism in GNNs. Their conclusion shows that attention helps GNNs generalize to larger and noisy graphs.

#### 7.1.3 Expressivity

On the expressivity of GNNs, Xu et al. (2019b), Morris et al. (2019) show that GCNs and GraphSAGE are less discriminative than

\begin{table}
\begin{tabular}{l l l} \hline Area & Application & References \\ \hline Graph Mining & Graph Matching & (Riba et al., 2018; Li et al., 2019b) \\  & Graph Clustering & (Zhang et al., 2019; Ying et al., 2018; Tsitsulin et al., 2020) \\ \hline Physics & Physical Systems Modeling & (Bhattaglia et al., 2016; Sukhbatar Ferguset et al., 2016; Wasters et al., 2017; Hohen, 2017; Kipf et al., 2018; Sanchez et al., 2018) \\ \hline Chemistry & Molecular Fingerprints & (Davenaud et al., 2015; Kearnes et al., 2016) \\  & Chemical Reaction & Do et al. (2019) \\  & Prediction & \\ \hline Biology & Protein Interface Prediction & Fort et al. (2017) \\  & Side Effects Prediction & ZÃ¼nik et al. (2018) \\  & Disease Classification & Rhee et al. (2018) \\ \hline Knowledge Graph & KB Completion & (Hamaguchi et al., 2017; Schlichtrull et al., 2018; Shang et al., 2019) \\  & KG Alignment & (Wang et al., 2018; Zhang et al., 2019; Xu et al., 2019c) \\ \hline Generation & Graph Generation & (Shahir et al., 2018; Nowak et al., 2018; Mei et al., 2018; You et al., 2018a, 2018b; De Cao and Kipf, 2018; Li et al., 2018; Shi et al., 2020; Liu et al., 2019; Grover et al., 2019) \\ \hline Combinatorial Optimization & Combinatorial Optimization & (Shahir et al., 2017; Nowak et al., 2018; Li et al., 2018; Koel et al., 2019; Bello et al., 2017; Vinyals et al., 2015; Sutton and Barto, 2018; Dai et al., 2016; Gase et al., 2019; Zheng et al., 2020; Selsum et al., 2019; Sato et al., 2019) \\ \hline Traffic Network & Traffic State Prediction & (Cui et al., 2018; Yu et al., 2018; Zheng et al., 2020; Guo et al., 2019) \\ \hline Recommendation & User-item Interaction & (van den Berg et al., 2017; Ying et al., 2018a) \\ Systems & Prediction & Social Recommendation & (Wu et al., 2019c; Fan et al., 2019) \\ \hline Others (Structural) & Stock Market & (Matsuunaga et al., 2019; Yang et al., 2019; Chen et al., 2018; Li et al., 2020; Kim et al., 2019) \\  & Software Defined Networks & Rusek et al. (2019) \\  & AMR Graph to Text & (Song et al., 2018; Baek et al., 2018) \\ \hline Text & Text Classification & (Wang et al., 2018; Yao et al., 2019; Zhang et al., 2018; Tai et al., 2015) \\  & Sequence Labeling & (Zhang et al., 2018; Markehessian and Titov, 2017) \\  & Neural Machine Translation & (Bhattaglia et al., 2017; Marchesgiani et al., 2016; Beck et al., 2018) \\  & Relation Extraction & (Miewan and Bassi, 2016; Zhou et al., 2017; Song et al., 2018b; Zhang et al., 2018) \\  & Event Extraction & (Nguyen and Grishman, 2018; Liu et al., 2019; Liu et al., 2020; Zhou et al., 2020) \\  & Fact Verification & (Zhou et al., 2019; Liu et al., 2020; Zhou et al., 2020) \\  & Question Answering & (Gong et al., 2018; De Cao et al., 2019; Qiu et al., 2019; Tu et al., 2019; Ding et al., 2019) \\  & Rational Reasoning & (Santoro et al., 2017; Palm et al., 2018; Battaglia et al., 2016) \\ \hline image & Social Relationship & Wang et al. (2018) \\  & Understanding & (Garcia and Bruna, 2018; Wang et al., 2018; Lee et al., 2018; Kampflingyer et al., 2019; Mamino et al., 2017) \\  & Image Classification & (Finey et al., 2017; Wang et al., 2018; Narsisirhan et al., 2018; Narsisirhan et al., 2018) \\  & Visual Question Answering & (Hu et al., 2018; Gu et al., 2018) \\  & Object Detection & (Qi et al., 2018; Jain et al., 2016) \\  & Interaction Detection & (Qi et al., 2018; Jain et al., 2016) \\  & Region Classification & Chen et al. (2018) \\  & Semantic Segmentation & (Zhang et al., 2016; Li et al., 2017) \\ \hline Other (Non-structural) & Program Verification & (Allamanis et al., 2016; Li et al., 2016) \\ \hline \end{tabular}
\end{table}
Table 3Applications of graph neural networks.

Weisfeiler-Leman (WL) test, an algorithm for graph isomorphism testing. Xu et al. (2019) also propose GINs for more expressive GNNs. Going beyond WL test, Barcelo et al. (2019) discuss if GNNs are expressible for \(FOC_{2}\), a fragment of first order logic. The authors find that existing GNNs can hardly fit the logic. For learning graph topologic structures, Garg et al. (2020) prove that locally dependent GNN variants are not capable to learn global graph properties, including diameters, biggest/smallest cycles, or motifs.

Lukas (2020) and Dehmamy et al. (2019) argue that existing works only consider the expressivity when GNNs have infinite layers and units. Their work investigates the representation power of GNNs with finite depth and width. Oono and Suzuki (2020) discuss the asymptotic behaviors of GNNs as the model deepens and model them as dynamic systems.

#### 7.1.4 Invariance

As there are no node orders in graphs, the output embeddings of GNNs are supposed to be permutation-invariant or equivariant to the input features. Maron et al. (2019) characterize permutation-invariant or equivariant linear layers to build invariant GNNs. Maron et al. (2019) further prove the result that the universal invariant GNNs can be obtained with higher-order tensorization. Keriven and Peyre (2019) provide an alternative proof and extend this conclusion to the equivariant case. Chen et al. (2019) build connections between permutation-invariance and graph isomorphism testing. To prove their equivalence, Chen et al. (2019) leverage sigma-algebra to describe the expressivity of GNNs.

#### 7.1.5 Transferability

A deterministic characteristic of GNNs is that the parameterization is united with graphs, which suggests the ability to transfer across graphs (so-called transferability) with performance guarantees. Levie et al. (2019) investigate the transferability of spectral graph filters, showing that such filters are able to transfer on graphs in the same domain. Ruiz et al. (2020) analyze GNN behaviour on graphons. Graphon refers to the limit of a sequence of graphs, which can also be seen as a generator for dense graphs. The authors conclude that GNNs are transferable across graphs obtained deterministically from the same graphon with different sizes.

#### 7.1.6 Label efficiency

(Semi-) Supervised learning for GNNs needs a considerable amount of labeled data to achieve a satisfying performance. Improving the label efficiency has been studied in the perspective of active learning, in which informative nodes are actively selected to be labeled by an oracle to train the GNNs. Cai et al. (2017), Gao et al. (2018), Hu et al. (2020) demonstrate that by selecting the informative nodes such as the high-degree nodes and uncertain nodes, the labeling efficiency can be dramatically improved.

### Empirical aspect

Besides theoretical analysis, empirical studies of GNNs are also required for better comparison and evaluation. Here we include several empirical studies for GNN evaluation and benchmarks.

#### 7.2.1 Evaluation

Evaluating machine learning models is an essential step in research. Concerns about experimental reproducibility and replicability have been raised over the years. Whether and to what extent do GNN models work? Which parts of the models contribute to the final performance? To investigate such fundamental questions, studies about fair evaluation strategies are urgently needed.

On semi-supervised node classification task, Shchur et al. (2018) explore how GNN models perform under same training strategies and hyperparameter tune. Their works concludes that different dataset splits lead to dramatically different rankings of models. Also, simple models could outperform complicated ones under proper settings. Errica et al. (2020) review several graph classification models and point out that they are compared improperly. Based on rigorous evaluation, structural information turns up to not be fully exploited for graph classification. You et al. (2020) discuss the architectural designs of GNN models, such as the number of layers and the aggregation function. By a huge amount of experiments, this work provides comprehensive guidelines for GNN designation over various tasks.

#### 7.2.2 Benchmarks

High-quality and large-scale benchmark datasets such as ImageNet are significant in machine learning research. However in graph learning, widely-adopted benchmarks are problematic. For example, most node classification datasets contain only 3000 to 20,000 nodes, which are small compared with real-world graphs. Furthermore, the experimental protocols across studies are not unified, which is hazardous to the literature. To mitigate this issue, Dwivedi et al. (2020), Hu et al. (2020) provide scalable and reliable benchmarks for graph learning. Dwivedi et al. (2020) build medium-scale benchmark datasets in multiple domains and tasks, while OGB (Hu et al., 2020) offers large-scale datasets. Furthermore, both works evaluate current GNN models and provide lea